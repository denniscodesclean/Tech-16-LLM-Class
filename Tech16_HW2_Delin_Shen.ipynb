{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community pypdf langchain-openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7pYMdfbHj53",
        "outputId": "34fc29a1-11b2-420b-d1a0-1d29fdbdf88a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/298.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.7/298.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.7/54.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.0/413.0 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_api')"
      ],
      "metadata": {
        "id": "fwgjnwoQJYFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a PDF\n",
        "!wget https://arxiv.org/pdf/2402.11753.pdf -q\n",
        "# Load PDF\n",
        "loader = PyPDFLoader(file_path = \"2402.11753.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "tGigT_s2ig2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.combine_documents.map_reduce import MapReduceDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "6kUZQ1NSA-Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stuff"
      ],
      "metadata": {
        "id": "8Y4OISmNlpiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stuff_summary(pages):\n",
        "  ## Define prompt\n",
        "  prompt_template = \"\"\"\n",
        "  Write a concise summary of the paper below, avoid using technical words, list the main points of the paper in bullet points, within 200 words.\n",
        "  ```{paper}```\n",
        "  \"\"\"\n",
        "  prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  # Define LLM chain\n",
        "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", api_key=open_ai_key)\n",
        "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "\n",
        "\n",
        "  # Define StuffDocumentsChain\n",
        "  stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"paper\")\n",
        "\n",
        "  stuff_result = stuff_chain.invoke(pages)\n",
        "\n",
        "  return stuff_result['output_text']"
      ],
      "metadata": {
        "id": "MVGKlYoilp7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stuff_summary(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "Ai_-eVeNmxks",
        "outputId": "08eb73d6-3204-4c66-bf17-a7389dd9ac5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"**Summary of the Paper:**\\n\\nThis paper discusses vulnerabilities in large language models (LLMs) related to safety measures. It introduces a new attack method called ArtPrompt, which uses ASCII art to bypass these safety protocols. The authors highlight that existing safety techniques focus only on the meaning of words, ignoring how they can be represented visually. This oversight allows malicious users to exploit LLMs by using ASCII art to mask harmful instructions. The paper presents a benchmark called the Vision-in-Text Challenge (VITC) to evaluate LLMs' ability to recognize ASCII art prompts. The results show that popular LLMs struggle with this task, making them susceptible to the ArtPrompt attack. The authors demonstrate that ArtPrompt can effectively induce unsafe behaviors in multiple LLMs, even when they are equipped with safety measures.\\n\\n**Main Points:**\\n- Safety in LLMs is crucial but currently inadequate.\\n- Existing safety measures focus only on word meanings.\\n- ASCII art can be used to bypass these safety measures.\\n- The Vision-in-Text Challenge (VITC) benchmark evaluates LLMs' recognition of ASCII art.\\n- Five state-of-the-art LLMs struggle with ASCII art prompts.\\n- ArtPrompt effectively induces unsafe behaviors in LLMs.\\n- The attack can be executed with minimal effort and without direct access to the models.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Paper:**\n",
        "\n",
        "This paper discusses vulnerabilities in large language models (LLMs) related to safety measures. It introduces a new attack method called ArtPrompt, which uses ASCII art to bypass these safety protocols. The authors highlight that existing safety techniques focus only on the meaning of words, ignoring how they can be represented visually. This oversight allows malicious users to exploit LLMs by using ASCII art to mask harmful instructions. The paper presents a benchmark called the Vision-in-Text Challenge (VITC) to evaluate LLMs' ability to recognize ASCII art prompts. The results show that popular LLMs struggle with this task, making them susceptible to the ArtPrompt attack. The authors demonstrate that ArtPrompt can effectively induce unsafe behaviors in multiple LLMs, even when they are equipped with safety measures.\n",
        "\n",
        "**Main Points:**\n",
        "- Safety in LLMs is crucial but currently inadequate.\n",
        "- Existing safety measures focus only on word meanings.\n",
        "- ASCII art can be used to bypass these safety measures.\n",
        "- The Vision-in-Text Challenge (VITC) benchmark evaluates LLMs' recognition of ASCII art.\n",
        "- Five state-of-the-art LLMs struggle with ASCII art prompts.\n",
        "- ArtPrompt effectively induces unsafe behaviors in LLMs.\n",
        "- The attack can be executed with minimal effort and without direct access to the models."
      ],
      "metadata": {
        "id": "P2ZB6KAEpAF5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Map Reduce"
      ],
      "metadata": {
        "id": "OPfTikuko9g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start LLM\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\", api_key=open_ai_key)\n",
        "\n",
        "# Prompt for each chunk\n",
        "map_prompt_template = \"\"\"\n",
        "Write a concise summary of the paper below, list the main points of the paper in bullet points, within 200 words.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "map_prompt = PromptTemplate.from_template(map_prompt_template)\n",
        "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
        "\n",
        "# Prompt for final sumamrization\n",
        "reduce_prompt_template = \"\"\"\n",
        "You are given multiple summaries from different sections of a document.\n",
        "Avoid using technical words, combine them and list the main points of the paper in bullet points, within 200 words.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "reduce_prompt = PromptTemplate.from_template(reduce_prompt_template)\n",
        "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
        "\n",
        "# Use StuffDocumentsChain to combine summaries for small chunks.\n",
        "combine_documents_chain = StuffDocumentsChain(\n",
        "    llm_chain=reduce_chain,\n",
        "    document_variable_name=\"text\"\n",
        ")\n",
        "\n",
        "# Chains\n",
        "map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Summary small chunks\n",
        "    llm_chain=map_chain,\n",
        "    # Combine summaries from above and generate final summary\n",
        "    reduce_documents_chain=combine_documents_chain,\n",
        "    document_variable_name=\"text\",\n",
        "    # Check what happened in between\n",
        "    return_intermediate_steps=True\n",
        ")\n",
        "\n",
        "map_result = map_reduce_chain.invoke(pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZS265y5o9Gf",
        "outputId": "5131eb78-c418-473a-e929-b97a784825b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-143e3c3c01e8>:28: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
            "  map_reduce_chain = MapReduceDocumentsChain(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "map_result['output_text']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "LFjP6c5cwHGa",
        "outputId": "748a7006-f4f2-40d6-ed5c-db2d51590f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"- The paper examines vulnerabilities in large language models (LLMs) related to their safety alignment, particularly focusing on ASCII art.\\n- It introduces ArtPrompt, a novel jailbreak attack that uses ASCII art to bypass safety measures and provoke harmful outputs from LLMs.\\n- The Vision-in-Text Challenge (VITC) is established to evaluate LLMs' ability to recognize non-semantic prompts, revealing significant limitations in five state-of-the-art models.\\n- ArtPrompt effectively masks sensitive words in prompts, allowing attackers to evade detection and induce unsafe behaviors.\\n- The study highlights that individual characters often lack meaning, and understanding relies more on their arrangement.\\n- Various defense mechanisms against jailbreak attacks are discussed, including detection and mitigation strategies.\\n- The effectiveness of ArtPrompt is demonstrated through extensive experiments, showing superior performance compared to existing methods.\\n- The paper emphasizes the need for improved safety measures in LLMs to prevent misuse and harmful outputs.\\n- Ethical considerations are raised regarding the potential for LLMs to generate instructions for illegal activities, underscoring the importance of robust defenses.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The paper examines vulnerabilities in large language models (LLMs) related to their safety alignment, particularly focusing on ASCII art.\n",
        "- It introduces ArtPrompt, a novel jailbreak attack that uses ASCII art to bypass safety measures and provoke harmful outputs from LLMs.\n",
        "- The Vision-in-Text Challenge (VITC) is established to evaluate LLMs' ability to recognize non-semantic prompts, revealing significant limitations in five state-of-the-art models.\n",
        "- ArtPrompt effectively masks sensitive words in prompts, allowing attackers to evade detection and induce unsafe behaviors.\n",
        "- The study highlights that individual characters often lack meaning, and understanding relies more on their arrangement.\n",
        "- Various defense mechanisms against jailbreak attacks are discussed, including detection and mitigation strategies.\n",
        "- The effectiveness of ArtPrompt is demonstrated through extensive experiments, showing superior performance compared to existing methods.\n",
        "- The paper emphasizes the need for improved safety measures in LLMs to prevent misuse and harmful outputs.\n",
        "- Ethical considerations are raised regarding the potential for LLMs to generate instructions for illegal activities, underscoring the importance of robust defenses."
      ],
      "metadata": {
        "id": "Ptf2ibcEw8W5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Intermediate Steps for Map Reducing"
      ],
      "metadata": {
        "id": "sjoSFwMGwz_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first intermediate couple steps\n",
        "for i, content in enumerate(map_result['intermediate_steps'][0:2]):\n",
        "    print(f\"Chunk # {i+1}:\")\n",
        "    print(content)\n",
        "    print(\"-\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzNAE5c2wIrA",
        "outputId": "330aa01e-c155-4311-f42d-779b270076ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk # 1:\n",
            "The paper \"ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs\" explores vulnerabilities in large language models (LLMs) related to their safety alignment. It highlights that existing safety measures primarily focus on semantic interpretation, neglecting alternative representations like ASCII art. The authors introduce a novel attack method, ArtPrompt, which exploits this oversight to bypass safety protocols and induce harmful outputs from LLMs.\n",
            "\n",
            "**Main Points:**\n",
            "- Safety is crucial for LLMs, with current techniques relying on semantic interpretation.\n",
            "- Real-world applications often use ASCII art, which poses a challenge for LLMs.\n",
            "- The paper presents the Vision-in-Text Challenge (VITC) to assess LLMs' ability to recognize non-semantic prompts.\n",
            "- Five state-of-the-art LLMs (GPT-3.5, GPT-4, Gemini, Claude, Llama2) struggle with ASCII art prompts.\n",
            "- ArtPrompt is developed as a jailbreak attack that effectively elicits undesired behaviors from LLMs.\n",
            "- The attack requires only black-box access to the LLMs, making it practical.\n",
            "- The authors provide a benchmark and code for further research on this vulnerability.\n",
            "--------------------------------------------------\n",
            "Chunk # 2:\n",
            "The paper discusses the interpretation of character arrangements in input spaces, emphasizing that the semantics of individual characters (like '*' and spaces) are often insufficient for meaningful understanding. Instead, it highlights the importance of the overall arrangement of characters, suggesting that human interpretation relies more on spatial organization than on the intrinsic meaning of the characters themselves.\n",
            "\n",
            "**Main Points:**\n",
            "- Individual characters often lack meaningful semantics in isolation.\n",
            "- Human interpretation is primarily based on the arrangement of characters.\n",
            "- The spatial organization of characters plays a crucial role in understanding input.\n",
            "- The paper suggests a shift in focus from character semantics to arrangement in computational contexts.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extrac Credit"
      ],
      "metadata": {
        "id": "lMfZCvSfHW7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEbKK525H1lP",
        "outputId": "a0dd5902-fa42-452a-d743-046f3065829d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "4rReIprDHYVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF\n",
        "loader_1 = PyPDFLoader(\"2402.11753.pdf\")\n",
        "pages_1 = loader.load()\n",
        "\n",
        "# Split into smaller chunks for better retrieval, this provides better split then load_and_split\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=6000, chunk_overlap=500)\n",
        "documents = text_splitter.split_documents(pages_1) # gives about 17 chunks"
      ],
      "metadata": {
        "id": "TFrsZLt_Ihj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=open_ai_key)\n",
        "\n",
        "# Store in ChromaDB\n",
        "vector_db = Chroma.from_documents(documents, embeddings)"
      ],
      "metadata": {
        "id": "S2mabJweJjVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db.get().keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ciz21ZdFJufJ",
        "outputId": "2dbd3541-1d8d-4039-a195-e71be2c7db76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['ids', 'embeddings', 'documents', 'uris', 'data', 'metadatas', 'included'])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Retrieve relevant document chunks\n",
        "retrieved_docs = vector_db.similarity_search(\"Summary this paper in 50 words?\", k=2)\n",
        "\n",
        "# Extract content from retrieved docs\n",
        "context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "# Define LLM and prompt\n",
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0, openai_api_key=open_ai_key)\n",
        "prompt_template = \"\"\"\n",
        "Write a concise summary of the paper below, avoid using technical words, list the main points of the paper in bullet points, within 200 words.\n",
        "  ```{context}```\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Run LLM, providing query and retrieved docs in prompt\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "summary = llm_chain.run({\"context\": context})\n",
        "\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ2j5INAKLxr",
        "outputId": "6463ba51-4874-4816-9fa9-14a73d0644c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Summary of the Paper:**\n",
            "\n",
            "The paper discusses vulnerabilities in large language models (LLMs) when interpreting training data solely based on semantics. It introduces a benchmark called the Vision-in-Text Challenge (VITC) to assess LLMs' ability to recognize prompts that should not be interpreted literally. The authors demonstrate that existing LLMs struggle with this task, leading to potential security risks. They also present a novel attack method, ArtPrompt, which exploits these vulnerabilities to provoke unsafe behaviors in LLMs.\n",
            "\n",
            "**Main Points:**\n",
            "\n",
            "- LLMs are vulnerable to attacks when trained only on semantic interpretations.\n",
            "- The Vision-in-Text Challenge (VITC) benchmark evaluates LLMs' recognition capabilities.\n",
            "- Five state-of-the-art LLMs were tested, revealing poor performance in recognizing specific prompts.\n",
            "- ArtPrompt is a new method designed to exploit these vulnerabilities effectively.\n",
            "- The study highlights the need for improved safety measures in LLM training and alignment.\n",
            "- Future work is suggested to explore the effectiveness of ArtPrompt on multimodal models.\n",
            "- The paper aims to enhance LLM safety and acknowledges the potential misuse of its findings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary of the Paper:**\n",
        "\n",
        "The paper discusses vulnerabilities in large language models (LLMs) when interpreting training data solely based on semantics. It introduces a benchmark called the Vision-in-Text Challenge (VITC) to assess LLMs' ability to recognize prompts that should not be interpreted literally. The authors demonstrate that existing LLMs struggle with this task, leading to potential security risks. They also present a novel attack method, ArtPrompt, which exploits these vulnerabilities to provoke unsafe behaviors in LLMs.\n",
        "\n",
        "**Main Points:**\n",
        "\n",
        "- LLMs are vulnerable to attacks when trained only on semantic interpretations.\n",
        "- The Vision-in-Text Challenge (VITC) benchmark evaluates LLMs' recognition capabilities.\n",
        "- Five state-of-the-art LLMs were tested, revealing poor performance in recognizing specific prompts.\n",
        "- ArtPrompt is a new method designed to exploit these vulnerabilities effectively.\n",
        "- The study highlights the need for improved safety measures in LLM training and alignment.\n",
        "- Future work is suggested to explore the effectiveness of ArtPrompt on multimodal models.\n",
        "- The paper aims to enhance LLM safety and acknowledges the potential misuse of its findings."
      ],
      "metadata": {
        "id": "jV40sXHMOtvs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aU3T6DKOOuOJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}